{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pm_query.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq8HUbomurbN"
      },
      "source": [
        "import yaml\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from Bio import Entrez"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_1lFOsU5_bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddfcaf5f-f189-4a8b-a3ff-72c4f85d6b9a"
      },
      "source": [
        "with open(\"apikeys.yaml\", \"r\") as yamlfile:\n",
        "  keys = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
        "  print(\"Read Successful\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNSii-U9vQ7c"
      },
      "source": [
        "# Handles the eSearch endpoint for Entrez\n",
        "def get_pmid(contact, key, term, **dates):\n",
        "    ''' Using the Entrez search term, it queries the eSearch endpoint of the Entrez api to retrieve the corresponding pmids and join them to the input df. '''\n",
        "    for_efetch = []\n",
        "    Entrez.email = contact\n",
        "    Entrez.api_key = key\n",
        "    \n",
        "    # Get total number of records\n",
        "    handle = Entrez.esearch(db='pubmed', term=term, retmax=1, mindate=dates.get(\"mindate\"), maxdate=dates.get(\"maxdate\"))\n",
        "    record = Entrez.read(handle)\n",
        "    count = int(record['Count'])\n",
        "\n",
        "    # Get all pmids with updated retmax\n",
        "    handle = Entrez.esearch(db='pubmed', term=term, retmax=count, mindate=dates.get(\"mindate\"), maxdate=dates.get(\"maxdate\"))\n",
        "    record = Entrez.read(handle)\n",
        "    for_efetch.append(record['IdList'])\n",
        "\n",
        "    # Change output from being a 1 item list\n",
        "    for_efetch = pd.Series(for_efetch[0]).str.split(pat=\",\", expand=True).values.tolist()\n",
        "\n",
        "    return for_efetch\n",
        "\n",
        "# Handles the eFetch endpoint for Entrez\n",
        "def get_data(pmid_list, contact, key):\n",
        "    ''' Using the pmids, it queries the eFetch endpoint to retrieve the details for the corresponding citation as a list of dictionaries. ''' \n",
        "    to_clean = []\n",
        "    counter = 0\n",
        "    for i in range(len(pmid_list)):\n",
        "            Entrez.email = contact\n",
        "            Entrez.api_key = key\n",
        "            handle = Entrez.efetch(db='pubmed', id=pmid_list[i], retmode='xml')\n",
        "            record = Entrez.read(handle)\n",
        "            to_clean.append(record)\n",
        "            if counter == 600:\n",
        "                print(f\"Number of records retrieved is {len(to_clean)}\")\n",
        "                time.sleep(60)\n",
        "                counter = 0\n",
        "            \n",
        "            counter += 1\n",
        "\n",
        "    return to_clean\n",
        "\n",
        "def clean_data(records):\n",
        "    ''' Using a list of dictionaries (that contains all citation data for the dataset), on a per citation basis, it extracts the following information about the citations where possible:\n",
        "    title, abstract, date, authors. The extracted information is saved as a list which is then converted into a df. \n",
        "    ''' \n",
        "    for record in records:\n",
        "        if record.get(\"PubmedArticle\") != []:\n",
        "            a = record[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"ArticleTitle\"]\n",
        "            if \"Abstract\" in record[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"].keys():\n",
        "                b = record[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"]\n",
        "            else:\n",
        "                b = []\n",
        "            if \"ArticleDate\" in record[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"].keys():\n",
        "                clean_date = pd.json_normalize(record[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"ArticleDate\"]).values.tolist()\n",
        "                clean_date = [item for sublist in clean_date for item in sublist]\n",
        "                c = \"-\".join(clean_date)\n",
        "            else:\n",
        "                c = []\n",
        "            if \"AuthorList\" in record[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"].keys():\n",
        "                clean_name = pd.json_normalize(record[\"PubmedArticle\"][0][\"MedlineCitation\"][\"Article\"][\"AuthorList\"])\n",
        "                if \"LastName\" in clean_name and \"ForeName\" in clean_name:\n",
        "                    clean_name = clean_name[\"LastName\"] + \" \" + clean_name[\"ForeName\"]\n",
        "                elif \"CollectiveName\" in clean_name:\n",
        "                    clean_name = clean_name[\"CollectiveName\"]\n",
        "                elif \"ForeName\" not in clean_name or \"CollectiveName\" not in clean_name:\n",
        "                    clean_name = clean_name[\"LastName\"]\n",
        "                elif \"LastName\" not in clean_name or \"CollectiveName\" not in clean_name:\n",
        "                    clean_name = clean_name[\"ForeName\"]\n",
        "                d = clean_name.values.tolist()\n",
        "            else:\n",
        "                d = []\n",
        "            e = record[\"PubmedArticle\"][0][\"MedlineCitation\"][\"PMID\"]\n",
        "        elif record.get(\"PubmedArticle\") == []:\n",
        "            a = record[\"PubmedBookArticle\"][0][\"BookDocument\"][\"Book\"][\"BookTitle\"]\n",
        "            if \"Abstract\" in record[\"PubmedBookArticle\"][0][\"BookDocument\"].keys():\n",
        "                b = record[\"PubmedBookArticle\"][0][\"BookDocument\"][\"Abstract\"][\"AbstractText\"]\n",
        "            else:\n",
        "                b = []\n",
        "            if \"PubDate\" in record[\"PubmedBookArticle\"][0][\"BookDocument\"][\"Book\"].keys():\n",
        "                clean_date = pd.json_normalize(record[\"PubmedBookArticle\"][0][\"BookDocument\"][\"Book\"][\"PubDate\"]).values.tolist()\n",
        "                clean_date = [item for sublist in clean_date for item in sublist]\n",
        "                c = \"-\".join(clean_date)\n",
        "            else:\n",
        "                c = []\n",
        "            if (\"AuthorList\" in record[\"PubmedBookArticle\"][0][\"BookDocument\"][\"Book\"].keys()) and (record[\"PubmedBookArticle\"][0][\"BookDocument\"][\"AuthorList\"] != []):\n",
        "                clean = record[\"PubmedBookArticle\"][0][\"BookDocument\"][\"AuthorList\"]\n",
        "                clean_name = [item for sublist in clean for item in sublist]\n",
        "                clean_name = pd.json_normalize(clean_name)\n",
        "                if \"LastName\" in clean_name and \"ForeName\" in clean_name:\n",
        "                    clean_name = clean_name[\"LastName\"] + \" \" + clean_name[\"ForeName\"]\n",
        "                elif \"CollectiveName\" in clean_name:\n",
        "                    clean_name = clean_name[\"CollectiveName\"]\n",
        "                elif \"ForeName\" not in clean_name or \"CollectiveName\" not in clean_name:\n",
        "                    clean_name = clean_name[\"LastName\"]\n",
        "                elif \"LastName\" not in clean_name or \"CollectiveName\" not in clean_name:\n",
        "                    clean_name = clean_name[\"ForeName\"]\n",
        "                d = clean_name.values.tolist()\n",
        "            else:\n",
        "                d = []\n",
        "            e = record[\"PubmedBookArticle\"][0][\"BookDocument\"][\"PMID\"]\n",
        "\n",
        "        v = [e,a,b,c,d]\n",
        "\n",
        "        data_tmp = pd.DataFrame(v).transpose().rename(columns={0:\"pmid\",1:\"title\",2:\"abstract\",3:\"date\",4:\"author(s)\"})\n",
        "\n",
        "        if records.index(record) == 0:\n",
        "            data = data_tmp\n",
        "        else:\n",
        "            data = pd.concat([data,data_tmp])\n",
        "\n",
        "    return data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFuEdgMkvVlc"
      },
      "source": [
        "email = \"rachit.sabharwal@uth.tmc.edu\"\n",
        "search = \"HIV\"\n",
        "hiv_pmids = get_pmid(contact=email, key=keys[\"apikeys\"][\"ncbikey\"][\"key\"], term=search, mindate=\"2020/01/01\", maxdate=\"2020/09/01\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('D:\\Dell_Desktop\\Documents\\Python Projects\\ph_1975_capstone_project\\webapp\\hiv_records.json', 'r') as outfile:\n",
        "   hiv_records = json.load(outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "hiv_clean = clean_data(hiv_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "hiv_clean = hiv_clean.reset_index(drop=True)\n",
        "hiv_clean.to_csv(\"hiv_records_clean.csv\", index=False)"
      ]
    }
  ]
}